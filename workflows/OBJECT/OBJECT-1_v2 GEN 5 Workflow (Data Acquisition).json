{
  "name": "OBJECT-1_v2 GEN 5 Workflow (Data Acquisition)",
  "nodes": [
    {
      "parameters": {},
      "id": "5d635fbd-abe1-4bd1-b249-502ed9929cb1",
      "name": "Manual Trigger - Input Researcher Data",
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [
        -1456,
        816
      ]
    },
    {
      "parameters": {
        "url": "https://serpapi.com/search",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpQueryAuth",
        "sendQuery": true,
        "queryParameters": {
          "parameters": [
            {
              "name": "engine",
              "value": "google"
            },
            {
              "name": "q",
              "value": "=site:deakin.edu.au \"{{ $json.researcher_name }}\" researcher"
            },
            {
              "name": "num",
              "value": "1"
            }
          ]
        },
        "options": {}
      },
      "id": "550dd79c-d213-4f06-82b7-9ebb3aed78e7",
      "name": "Source 1: Search Deakin Profile",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [
        896,
        -16
      ],
      "credentials": {
        "httpQueryAuth": {
          "id": "gQcmQB1eL9FJrs0y",
          "name": "SerpAPI"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Extract Deakin profile URL from search results\nconst results = $input.item.json.organic_results || [];\n\nif (results.length === 0) {\n  return {\n    _node: 'Extract Deakin Profile URL',\n    deakin_found: false,\n    deakin_profile_url: null,\n    deakin_data: null\n  };\n}\n\nconst firstResult = results[0];\n\nreturn {\n  _node: 'Extract Deakin Profile URL',\n  deakin_found: true,\n  deakin_profile_url: firstResult.link,\n  deakin_title: firstResult.title,\n  deakin_snippet: firstResult.snippet\n};"
      },
      "id": "8c425151-d65a-45c3-9bdc-4f44ee1a04e6",
      "name": "Extract Deakin Profile URL",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1104,
        -16
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://134.199.152.159:8001/scrape",
        "sendBody": true,
        "bodyParameters": {
          "parameters": [
            {
              "name": "url",
              "value": "={{ $json.deakin_profile_url }}"
            }
          ]
        },
        "options": {}
      },
      "id": "e00be966-4b1c-4ee5-868e-1ee388e03d77",
      "name": "Crawl4AI: Scrape Deakin Profile",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [
        1296,
        -16
      ],
      "executeOnce": false,
      "alwaysOutputData": true,
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "url": "=https://pub.orcid.org/v3.0/search?q={{ $json.researcher_name }}",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Accept",
              "value": "application/json"
            }
          ]
        },
        "options": {}
      },
      "id": "136ece61-e091-4bf7-b288-52c1c084e01d",
      "name": "Search ORCID by Name",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [
        896,
        192
      ],
      "executeOnce": false,
      "alwaysOutputData": true,
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "jsCode": "// Extract ORCID ID from search results\nconst orcidId = $input.item.json.orcid_id;\n\nif (orcidId) {\n  // Already have ORCID ID from input\n  return { orcid_id: orcidId };\n}\n\nconst results = $input.item.json.result || [];\n\nif (results.length === 0) {\n  return {\n    orcid_found: false,\n    orcid_id: null\n  };\n}\n\n// Take first result\nconst firstResult = results[0];\nconst orcidIdFromSearch = firstResult['orcid-identifier']?.path;\n\nreturn {\n  orcid_found: true,\n  orcid_id: orcidIdFromSearch\n};"
      },
      "id": "5e2c6133-50d2-43a4-a092-477121f8ae6e",
      "name": "Extract ORCID ID",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1104,
        192
      ]
    },
    {
      "parameters": {
        "url": "=https://pub.orcid.org/v3.0/{{ $json.orcid_id }}/record",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Accept",
              "value": "application/json"
            }
          ]
        },
        "options": {}
      },
      "id": "11b72e08-87ae-4909-8b20-3122df942f88",
      "name": "Source 2: Fetch ORCID Full Record",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [
        1296,
        192
      ],
      "executeOnce": false,
      "alwaysOutputData": true,
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "jsCode": "// Parse ORCID record with timestamp metadata\nconst orcid = $input.item.json;\nconst person = orcid.person || {};\nconst activities = orcid['activities-summary'] || {};\n\nfunction safeExtract(obj, path, defaultValue = null) {\n  return path.split('.').reduce((acc, part) => acc?.[part], obj) ?? defaultValue;\n}\n\nconst orcidId = safeExtract(orcid, 'orcid-identifier.path');\n\nreturn {\n  _node: 'Parse ORCID Data',\n  orcid_found: !!orcidId,\n  orcid_id: orcidId,\n  \n  given_name: safeExtract(person, 'name.given-names.value'),\n  family_name: safeExtract(person, 'name.family-name.value'),\n  keywords: person.keywords?.keyword?.map(k => k.content) || [],\n  \n  publications: activities.works?.group?.slice(0, 50).map(work => {\n    const summary = work['work-summary']?.[0] || {};\n    return {\n      title: safeExtract(summary, 'title.title.value'),\n      year: safeExtract(summary, 'publication-date.year.value'),\n      type: summary.type,\n      doi: summary['external-ids']?.['external-id']?.find(id => id['external-id-type'] === 'doi')?.['external-id-value']\n    };\n  }) || [],\n  \n  employment: activities.employments?.['affiliation-group']?.map(emp => {\n    const summary = emp.summaries?.[0]?.['employment-summary'] || {};\n    return {\n      organization: safeExtract(summary, 'organization.name'),\n      role: summary['role-title'],\n      department: summary['department-name'],\n      start_year: safeExtract(summary, 'start-date.year.value'),\n      end_year: safeExtract(summary, 'end-date.year.value')\n    };\n  }) || [],\n  \n  education: activities.educations?.['affiliation-group']?.map(edu => {\n    const summary = edu.summaries?.[0]?.['education-summary'] || {};\n    return {\n      institution: safeExtract(summary, 'organization.name'),\n      degree: summary['role-title'],\n      year: safeExtract(summary, 'end-date.year.value')\n    };\n  }) || [],\n  \n  funding_count: activities.fundings?.group?.length || 0,\n  \n  _source_meta: {\n    source: 'orcid',\n    scraped_at: new Date().toISOString(),\n    success: !!orcidId,\n    url: orcidId ? `https://orcid.org/${orcidId}` : null\n  }\n};"
      },
      "id": "f9dc24c3-7bfc-4c91-8633-39ccd194c885",
      "name": "Parse ORCID Data",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1488,
        192
      ]
    },
    {
      "parameters": {
        "url": "=https://api.elsevier.com/content/author/orcid/{{ $json.orcid_id }}?view=ENHANCED",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Accept",
              "value": "application/json"
            },
            {
              "name": "X-ELS-APIKey",
              "value": "b44b780c60c6b6731bb436c65f9ea233"
            },
            {
              "name": "X-ELS-Insttoken",
              "value": "8b6097ed3698daa5cc21857a67ee1a29"
            }
          ]
        },
        "options": {}
      },
      "id": "22cf9596-0728-4b71-9dc6-456df1e3528b",
      "name": "Search Scopus by ORCID",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [
        896,
        384
      ],
      "executeOnce": false,
      "alwaysOutputData": true,
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "jsCode": "// Parse Scopus author metrics with timestamp metadata\nconst response = $input.item.json['author-retrieval-response']?.[0];\n\nif (!response) {\n  return { \n    _node: 'Parse Scopus Data',\n    scopus_found: false,\n    _source_meta: {\n      source: 'scopus',\n      scraped_at: new Date().toISOString(),\n      success: false,\n      error: 'No author data returned'\n    }\n  };\n}\n\nconst coredata = response.coredata || {};\nconst hIndex = response['h-index'] || 0;\nconst subjectAreas = response['subject-areas']?.['subject-area'] || [];\nconst affilCurrent = response['affiliation-current']?.affiliation;\nconst affilHistory = response['affiliation-history']?.affiliation || [];\n\nconst scopusId = coredata['dc:identifier']?.replace('AUTHOR_ID:', '');\n\nreturn {\n  _node: 'Parse Scopus Data',\n  scopus_found: true,\n  scopus_id: scopusId,\n  \n  document_count: parseInt(coredata['document-count']) || 0,\n  citation_count: parseInt(coredata['citation-count']) || 0,\n  cited_by_count: parseInt(coredata['cited-by-count']) || 0,\n  h_index: parseInt(hIndex) || 0,\n  \n  subject_areas: subjectAreas.map(s => ({\n    area: s['$'],\n    code: s['@code'],\n    abbrev: s['@abbrev']\n  })),\n  \n  affiliation_current: {\n    name: affilCurrent?.['ip-doc']?.afdispname,\n    city: affilCurrent?.['ip-doc']?.['address']?.city,\n    country: affilCurrent?.['ip-doc']?.['address']?.country\n  },\n  \n  affiliation_history: Array.isArray(affilHistory) \n    ? affilHistory.map(aff => ({\n        name: aff['ip-doc']?.afdispname,\n        city: aff['ip-doc']?.['address']?.city,\n        country: aff['ip-doc']?.['address']?.country\n      }))\n    : [{\n        name: affilHistory['ip-doc']?.afdispname,\n        city: affilHistory['ip-doc']?.['address']?.city,\n        country: affilHistory['ip-doc']?.['address']?.country\n      }],\n  \n  _source_meta: {\n    source: 'scopus',\n    scraped_at: new Date().toISOString(),\n    success: true,\n    url: scopusId ? `https://www.scopus.com/authid/detail.uri?authorId=${scopusId}` : null\n  }\n};"
      },
      "id": "2401ed0b-6722-4c94-bf39-3f90419c15f9",
      "name": "Parse Scopus Data",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1504,
        384
      ]
    },
    {
      "parameters": {
        "url": "https://serpapi.com/search",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpQueryAuth",
        "sendQuery": true,
        "queryParameters": {
          "parameters": [
            {
              "name": "engine",
              "value": "google_scholar_author"
            },
            {
              "name": "author_id",
              "value": "={{ $json.scholar_id }}"
            },
            {
              "name": "num",
              "value": "100"
            }
          ]
        },
        "options": {}
      },
      "id": "4bcbab41-c969-4c74-a6a4-9f1f30953e72",
      "name": "Source 4: Fetch Google Scholar Profile",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [
        912,
        576
      ],
      "executeOnce": false,
      "alwaysOutputData": true,
      "credentials": {
        "httpQueryAuth": {
          "id": "gQcmQB1eL9FJrs0y",
          "name": "SerpAPI"
        }
      },
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "jsCode": "// Parse Google Scholar profile with timestamp metadata\nconst scholarResponse = $input.item.json;\nconst scholar = Array.isArray(scholarResponse) ? scholarResponse[0] : scholarResponse;\n\nif (!scholar.author) {\n  return { \n    _node: 'Parse Google Scholar Data',\n    scholar_found: false,\n    _source_meta: {\n      source: 'scholar',\n      scraped_at: new Date().toISOString(),\n      success: false,\n      error: 'No author profile found'\n    }\n  };\n}\n\nconst author = scholar.author;\nconst citedBy = scholar.cited_by || {};\nconst citedByTable = citedBy.table || [];\nconst scholarId = scholar.search_parameters?.author_id;\n\nreturn {\n  _node: 'Parse Google Scholar Data',\n  scholar_found: true,\n  scholar_id: scholarId,\n  \n  name: author.name,\n  affiliation: author.affiliations,\n  email: author.email,\n  interests: (author.interests || []).map(i => i.title),\n  homepage: author.website || null,\n  \n  cited_by_all: citedByTable[0]?.citations?.all || 0,\n  cited_by_recent: citedByTable[0]?.citations?.since_2020 || 0,\n  h_index_all: citedByTable[1]?.h_index?.all || 0,\n  h_index_recent: citedByTable[1]?.h_index?.since_2020 || 0,\n  i10_index_all: citedByTable[2]?.i10_index?.all || 0,\n  i10_index_recent: citedByTable[2]?.i10_index?.since_2020 || 0,\n  \n  publications: (scholar.articles || []).slice(0, 50).map(article => ({\n    title: article.title,\n    link: article.link,\n    citation_id: article.citation_id,\n    authors: article.authors,\n    publication: article.publication,\n    cited_by: article.cited_by?.value || 0,\n    year: article.year\n  })),\n  \n  co_authors: (scholar.co_authors || []).map(co => ({\n    name: co.name,\n    affiliation: co.affiliations,\n    author_id: co.author_id,\n    thumbnail: co.thumbnail\n  })),\n  \n  citations_per_year: citedBy.graph || [],\n  \n  _source_meta: {\n    source: 'scholar',\n    scraped_at: new Date().toISOString(),\n    success: true,\n    url: scholarId ? `https://scholar.google.com/citations?user=${scholarId}` : null\n  }\n};"
      },
      "id": "5888120e-5127-4659-9ff6-8c5ff4a15523",
      "name": "Parse Google Scholar Data",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1440,
        576
      ]
    },
    {
      "parameters": {
        "url": "https://serpapi.com/search",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpQueryAuth",
        "sendQuery": true,
        "queryParameters": {
          "parameters": [
            {
              "name": "engine",
              "value": "google_patents"
            },
            {
              "name": "inventor",
              "value": "=\"={{ $json.researcher_name }}\""
            },
            {
              "name": "num",
              "value": "100"
            }
          ]
        },
        "options": {}
      },
      "id": "cb1db396-0632-49b6-8a26-a43ad60b6c39",
      "name": "Source 5: Search Google Patents",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [
        896,
        784
      ],
      "executeOnce": false,
      "alwaysOutputData": true,
      "credentials": {
        "httpQueryAuth": {
          "id": "gQcmQB1eL9FJrs0y",
          "name": "SerpAPI"
        }
      },
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "jsCode": "// Parse Google Patents with timestamp metadata\nconst patents = $input.item.json.organic_results || [];\n\nconst allIpcCodes = patents.flatMap(p => p.classifications?.ipc || []);\nconst uniqueIpcCodes = [...new Set(allIpcCodes)];\n\nconst allAssignees = patents.map(p => p.assignee).filter(Boolean);\nconst uniqueAssignees = [...new Set(allAssignees)];\n\nreturn {\n  _node: 'Parse Patent Data',\n  patent_count: patents.length,\n  has_patents: patents.length > 0,\n  \n  patents: patents.map(patent => ({\n    title: patent.title,\n    patent_id: patent.patent_id,\n    link: patent.link,\n    filing_date: patent.filing_date,\n    grant_date: patent.grant_date,\n    priority_date: patent.priority_date,\n    publication_date: patent.publication_date,\n    ipc_classification: patent.classifications?.ipc || [],\n    cpc_classification: patent.classifications?.cpc || [],\n    inventor: patent.inventor,\n    assignee: patent.assignee,\n    snippet: patent.snippet,\n    status: patent.status,\n    pdf: patent.pdf,\n    thumbnail: patent.thumbnail\n  })),\n  \n  innovation_signal: patents.length > 0 ? 'high' : 'low',\n  primary_ipc_codes: uniqueIpcCodes.slice(0, 20),\n  primary_assignees: uniqueAssignees.slice(0, 10),\n  granted_count: patents.filter(p => p.status === 'granted').length,\n  pending_count: patents.filter(p => p.status === 'pending').length,\n  \n  _source_meta: {\n    source: 'patents',\n    scraped_at: new Date().toISOString(),\n    success: patents.length > 0,\n    url: 'https://patents.google.com/'\n  }\n};"
      },
      "id": "9b94a3d6-dd74-40d8-aa55-6ba3b4765014",
      "name": "Parse Patent Data",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1104,
        784
      ]
    },
    {
      "parameters": {
        "jsCode": "// ============================================================\n// Node: Merge All 5 Sources (MODIFIED for Maintenance Scan)\n// Purpose: Combine data from all 5 sources + compute field hashes\n// Input: Results from Deakin, ORCID, Scopus, Scholar, Patents\n// Output: Merged researcher object with hashes (and change detection if maintenance scan)\n// ============================================================\n\n// ============================================================\n// SECTION 1: Get input from all source nodes\n// ============================================================\n\n// Get the mode and context from earlier in the pipeline\nlet mode = 'initial_acquisition';\nlet existingFieldHashes = {};\nlet assetId = null;\nlet scanInitiatedAt = null;\n\n// Try to get maintenance scan context\ntry {\n  const maintenanceContext = $('Prepare Maintenance Scan').first().json;\n  if (maintenanceContext && maintenanceContext.mode === 'maintenance_scan') {\n    mode = 'maintenance_scan';\n    existingFieldHashes = maintenanceContext.existing_field_hashes || {};\n    assetId = maintenanceContext.asset_id;\n    scanInitiatedAt = maintenanceContext.scan_initiated_at;\n  }\n} catch (e) {\n  // Not a maintenance scan - continue with initial acquisition\n}\n\n// Try to get context from Format Researcher Data (initial acquisition path)\ntry {\n  if (mode !== 'maintenance_scan') {\n    const formatContext = $('Format Researcher Data').first().json;\n    assetId = formatContext.asset_id || `fat:researcher:${(formatContext.researcher_name || 'unknown').toLowerCase().replace(/\\s+/g, '_')}`;\n  }\n} catch (e) {\n  // Continue without format context\n}\n\n// Get source data - handle both array and single item responses\nfunction getSourceData(nodeName) {\n  try {\n    const items = $(nodeName).all();\n    if (items.length === 0) return null;\n    return items[0].json;\n  } catch (e) {\n    return null;\n  }\n}\n\nconst deakinData = getSourceData('Parse Crawl4AI Output') || {};\nconst orcidData = getSourceData('Parse ORCID Data') || {};\nconst scopusData = getSourceData('Parse Scopus Data') || {};\nconst scholarData = getSourceData('Parse Scholar Data') || {};\nconst patentsData = getSourceData('Parse Patents Data') || {};\n\n// ============================================================\n// SECTION 2: Title similarity function for deduplication\n// ============================================================\n\nfunction titleSimilarity(title1, title2) {\n  if (!title1 || !title2) return 0;\n  \n  const words1 = new Set(title1.toLowerCase().split(/\\s+/).filter(w => w.length > 2));\n  const words2 = new Set(title2.toLowerCase().split(/\\s+/).filter(w => w.length > 2));\n  \n  if (words1.size === 0 || words2.size === 0) return 0;\n  \n  const intersection = new Set([...words1].filter(x => words2.has(x)));\n  const union = new Set([...words1, ...words2]);\n  \n  return intersection.size / union.size;\n}\n\n// ============================================================\n// SECTION 3: Build merged researcher object\n// ============================================================\n\nconst merged = {\n  // Core identifiers\n  asset_id: assetId,\n  name: orcidData.given_name && orcidData.family_name \n    ? `${orcidData.given_name} ${orcidData.family_name}`\n    : deakinData.name || scholarData.name || 'Unknown',\n  orcid: orcidData.orcid_id || null,\n  email: deakinData.email || null,\n  institution: deakinData.institution || scopusData.affiliation_current?.name || 'Unknown',\n  \n  // Collections to be populated\n  publications: [],\n  collaborators: [],\n  keywords: [],\n  affiliations: [],\n  patents: [],\n  metrics: {},\n  data_quality_flags: [],\n  \n  // Metadata\n  mode: mode,\n  merged_at: new Date().toISOString()\n};\n\n// ============================================================\n// SECTION 4: Merge publications from all sources\n// ============================================================\n\nconst publicationsMap = new Map();\n\n// Add Scopus publications\n(scopusData.publications || []).forEach(pub => {\n  const doi = pub.doi;\n  const key = doi || `scopus_${(pub.title || '').toLowerCase().substring(0, 50)}`;\n  \n  if (key && !publicationsMap.has(key)) {\n    publicationsMap.set(key, {\n      title: pub.title,\n      doi: doi,\n      year: pub.year,\n      citations: pub.citations || 0,\n      authors: pub.authors || [],\n      abstract: pub.abstract,\n      keywords: pub.keywords || [],\n      source: 'scopus'\n    });\n  }\n});\n\n// Add ORCID publications\n(orcidData.publications || []).forEach(pub => {\n  const doi = pub.doi;\n  const key = doi || `orcid_${(pub.title || '').toLowerCase().substring(0, 50)}`;\n  \n  if (doi && publicationsMap.has(doi)) {\n    // Merge with existing\n    const existing = publicationsMap.get(doi);\n    existing.citations = Math.max(existing.citations || 0, pub.citations || 0);\n  } else if (!publicationsMap.has(key)) {\n    // Check for title duplicates\n    let isDuplicate = false;\n    for (const [existingKey, existingPub] of publicationsMap.entries()) {\n      if (titleSimilarity(pub.title, existingPub.title) > 0.85) {\n        isDuplicate = true;\n        break;\n      }\n    }\n    \n    if (!isDuplicate && pub.title) {\n      publicationsMap.set(key, {\n        title: pub.title,\n        doi: doi,\n        year: pub.year,\n        citations: pub.citations || 0,\n        type: pub.type,\n        source: 'orcid'\n      });\n    }\n  }\n});\n\n// Add Scholar publications\n(scholarData.publications || scholarData.articles || []).forEach(pub => {\n  const title = pub.title || '';\n  const doi = pub.doi;\n  \n  if (doi && publicationsMap.has(doi)) {\n    // Merge citations (Scholar often has higher counts)\n    const existing = publicationsMap.get(doi);\n    existing.citations = Math.max(existing.citations || 0, pub.citations || pub.cited_by || 0);\n  } else {\n    // Check for title duplicates\n    let isDuplicate = false;\n    for (const [existingKey, existingPub] of publicationsMap.entries()) {\n      if (titleSimilarity(title, existingPub.title) > 0.85) {\n        isDuplicate = true;\n        // Update citation count if higher\n        existingPub.citations = Math.max(existingPub.citations || 0, pub.citations || pub.cited_by || 0);\n        break;\n      }\n    }\n    \n    if (!isDuplicate && title) {\n      const key = doi || `scholar_${title.toLowerCase().substring(0, 50)}`;\n      publicationsMap.set(key, {\n        title: title,\n        doi: doi,\n        year: pub.year,\n        citations: pub.citations || pub.cited_by || 0,\n        source: 'scholar'\n      });\n    }\n  }\n});\n\nmerged.publications = Array.from(publicationsMap.values());\n\n// ============================================================\n// SECTION 5: Extract collaborators\n// ============================================================\n\nconst collaboratorsSet = new Set();\nconst researcherNameLower = merged.name.toLowerCase();\n\n// From publications\nmerged.publications.forEach(pub => {\n  (pub.authors || []).forEach(author => {\n    const authorName = (typeof author === 'string' ? author : author.name || '').trim();\n    if (authorName && authorName.toLowerCase() !== researcherNameLower) {\n      collaboratorsSet.add(authorName);\n    }\n  });\n});\n\n// From Scholar co-authors\n(scholarData.co_authors || []).forEach(coauthor => {\n  const name = (typeof coauthor === 'string' ? coauthor : coauthor.name || '').trim();\n  if (name && name.toLowerCase() !== researcherNameLower) {\n    collaboratorsSet.add(name);\n  }\n});\n\nmerged.collaborators = Array.from(collaboratorsSet);\n\n// ============================================================\n// SECTION 6: Merge keywords from all sources\n// ============================================================\n\nconst keywordsSet = new Set();\n\n// From university/Deakin data\n(deakinData.research_interests || deakinData.keywords || []).forEach(kw => {\n  if (kw) keywordsSet.add(kw.toLowerCase().trim());\n});\n\n// From ORCID\n(orcidData.keywords || []).forEach(kw => {\n  if (kw) keywordsSet.add(kw.toLowerCase().trim());\n});\n\n// From Scholar interests\n(scholarData.interests || []).forEach(kw => {\n  if (kw) keywordsSet.add(kw.toLowerCase().trim());\n});\n\n// From Scopus subject areas\n(scopusData.subject_areas || []).forEach(area => {\n  const areaName = area.area || area;\n  if (areaName) keywordsSet.add(areaName.toLowerCase().trim());\n});\n\n// From publication keywords\nmerged.publications.forEach(pub => {\n  (pub.keywords || []).forEach(kw => {\n    if (kw) keywordsSet.add(kw.toLowerCase().trim());\n  });\n});\n\nmerged.keywords = Array.from(keywordsSet);\n\n// ============================================================\n// SECTION 7: Merge affiliations\n// ============================================================\n\nconst affiliations = [];\n\n// From ORCID employment\n(orcidData.employment || []).forEach(emp => {\n  const orgName = typeof emp.organization === 'string' \n    ? emp.organization \n    : emp.organization?.name;\n  \n  if (orgName) {\n    affiliations.push({\n      institution: orgName,\n      department: emp.department || null,\n      role: emp.role || emp['role-title'] || null,\n      start_date: emp.start_year || emp.start_date || null,\n      end_date: emp.end_year || emp.end_date || null,\n      current: !emp.end_year && !emp.end_date,\n      source: 'orcid'\n    });\n  }\n});\n\n// From Scopus affiliation history\n(scopusData.affiliation_history || []).forEach(aff => {\n  if (aff.name) {\n    affiliations.push({\n      institution: aff.name,\n      city: aff.city,\n      country: aff.country,\n      source: 'scopus'\n    });\n  }\n});\n\n// Current affiliation from Scopus\nif (scopusData.affiliation_current?.name) {\n  const currentExists = affiliations.some(a => \n    a.institution === scopusData.affiliation_current.name && a.current\n  );\n  if (!currentExists) {\n    affiliations.push({\n      institution: scopusData.affiliation_current.name,\n      city: scopusData.affiliation_current.city,\n      country: scopusData.affiliation_current.country,\n      current: true,\n      source: 'scopus'\n    });\n  }\n}\n\n// From Deakin/university data\nif (deakinData.department || deakinData.position) {\n  affiliations.push({\n    institution: merged.institution,\n    department: deakinData.department || null,\n    position: deakinData.position || null,\n    current: true,\n    source: 'university'\n  });\n}\n\nmerged.affiliations = affiliations;\n\n// ============================================================\n// SECTION 8: Patents\n// ============================================================\n\nmerged.patents = patentsData.patents || [];\n\n// ============================================================\n// SECTION 9: Calculate metrics\n// ============================================================\n\nmerged.metrics = {\n  h_index: Math.max(\n    scopusData.h_index || 0,\n    scholarData.h_index || scholarData.cited_by?.h_index || 0\n  ),\n  total_citations: Math.max(\n    scopusData.citation_count || scopusData.cited_by_count || 0,\n    scholarData.citations || scholarData.cited_by?.all || 0\n  ),\n  publication_count: Math.max(\n    scopusData.document_count || 0,\n    merged.publications.length\n  ),\n  collaborator_count: merged.collaborators.length,\n  patent_count: patentsData.patent_count || merged.patents.length || 0,\n  keyword_count: merged.keywords.length,\n  affiliation_count: merged.affiliations.length\n};\n\n// ============================================================\n// SECTION 10: Data quality flags\n// ============================================================\n\nif (!merged.orcid) merged.data_quality_flags.push('missing_orcid');\nif (merged.metrics.publication_count === 0) merged.data_quality_flags.push('no_publications');\nif (merged.affiliations.length === 0) merged.data_quality_flags.push('no_affiliations');\nif (merged.keywords.length === 0) merged.data_quality_flags.push('no_keywords');\n\n// Calculate completeness score\nlet completeness = 0;\nif (merged.orcid) completeness += 20;\nif (merged.metrics.publication_count > 0) completeness += 30;\nif (merged.affiliations.length > 0) completeness += 20;\nif (merged.metrics.h_index > 0) completeness += 15;\nif (merged.keywords.length > 0) completeness += 10;\nif (merged.collaborators.length > 0) completeness += 5;\n\nmerged.data_completeness = completeness;\n\n// ============================================================\n// SECTION 11: Compute field hashes for change detection\n// ============================================================\n\nfunction computeHash(value) {\n  // Simple but effective hash for comparison\n  const str = JSON.stringify(value, Object.keys(value || {}).sort());\n  let hash = 0;\n  for (let i = 0; i < str.length; i++) {\n    const char = str.charCodeAt(i);\n    hash = ((hash << 5) - hash) + char;\n    hash = hash & hash; // Convert to 32-bit integer\n  }\n  return Math.abs(hash).toString(16).padStart(8, '0');\n}\n\n// Compute hashes for key fields\nconst newFieldHashes = {\n  publications: computeHash({\n    count: merged.publications.length,\n    titles: merged.publications.map(p => p.title).sort()\n  }),\n  h_index: computeHash(merged.metrics.h_index),\n  total_citations: computeHash(merged.metrics.total_citations),\n  affiliations: computeHash(merged.affiliations.map(a => a.institution).sort()),\n  collaborators: computeHash(merged.collaborators.sort().slice(0, 50)),\n  keywords: computeHash(merged.keywords.sort()),\n  patent_count: computeHash(merged.metrics.patent_count),\n  publication_count: computeHash(merged.metrics.publication_count)\n};\n\nmerged.field_hashes = newFieldHashes;\n\n// ============================================================\n// SECTION 12: Change detection (maintenance scan only)\n// ============================================================\n\nif (mode === 'maintenance_scan') {\n  const changedFields = [];\n  const changeDetails = {};\n  \n  for (const [field, newHash] of Object.entries(newFieldHashes)) {\n    const existingHash = existingFieldHashes[field];\n    \n    if (existingHash && existingHash !== newHash) {\n      changedFields.push(field);\n      changeDetails[field] = {\n        previous_hash: existingHash,\n        new_hash: newHash,\n        changed: true\n      };\n    } else if (!existingHash) {\n      // New field not previously tracked\n      changeDetails[field] = {\n        previous_hash: null,\n        new_hash: newHash,\n        changed: false,\n        note: 'field_not_previously_tracked'\n      };\n    } else {\n      changeDetails[field] = {\n        previous_hash: existingHash,\n        new_hash: newHash,\n        changed: false\n      };\n    }\n  }\n  \n  // Determine scan status\n  const scanStatus = changedFields.length > 0 ? 'changes_detected' : 'verified_unchanged';\n  \n  console.log(`ðŸ” Scan complete: ${scanStatus}`);\n  console.log(`ðŸ“Š Changed fields: ${changedFields.length > 0 ? changedFields.join(', ') : 'none'}`);\n  \n  merged.scan_result = {\n    scan_status: scanStatus,\n    changed_fields: changedFields,\n    change_count: changedFields.length,\n    change_details: changeDetails,\n    new_field_hashes: newFieldHashes,\n    existing_field_hashes: existingFieldHashes,\n    scanned_at: new Date().toISOString(),\n    scan_initiated_at: scanInitiatedAt,\n    scan_duration_ms: scanInitiatedAt \n      ? new Date().getTime() - new Date(scanInitiatedAt).getTime() \n      : null\n  };\n  \n  // Include fresh data if changes detected (for GOVERN-3)\n  if (scanStatus === 'changes_detected') {\n    merged.fresh_data = {\n      publications: merged.publications,\n      metrics: merged.metrics,\n      affiliations: merged.affiliations,\n      collaborators: merged.collaborators,\n      keywords: merged.keywords,\n      patents: merged.patents\n    };\n  }\n}\n\n// ============================================================\n// SECTION 13: Source metadata\n// ============================================================\n\nmerged.source_metadata = {\n  sources_scraped: [\n    deakinData._source_meta ? 'deakin' : null,\n    orcidData._source_meta ? 'orcid' : null,\n    scopusData._source_meta ? 'scopus' : null,\n    scholarData._source_meta ? 'scholar' : null,\n    patentsData._source_meta ? 'patents' : null\n  ].filter(Boolean),\n  \n  scrape_timestamps: {\n    deakin: deakinData._source_meta?.scraped_at,\n    orcid: orcidData._source_meta?.scraped_at,\n    scopus: scopusData._source_meta?.scraped_at,\n    scholar: scholarData._source_meta?.scraped_at,\n    patents: patentsData._source_meta?.scraped_at\n  },\n  \n  source_success: {\n    deakin: deakinData._source_meta?.success || false,\n    orcid: orcidData._source_meta?.success || orcidData.orcid_found || false,\n    scopus: scopusData._source_meta?.success || scopusData.scopus_found || false,\n    scholar: scholarData._source_meta?.success || scholarData.scholar_found || false,\n    patents: patentsData._source_meta?.success || false\n  }\n};\n\n// ============================================================\n// SECTION 14: Return result\n// ============================================================\n\nconsole.log(`Merge complete for: ${merged.name}`);\nconsole.log(`Publications: ${merged.metrics.publication_count}, H-index: ${merged.metrics.h_index}`);\nconsole.log(`ðŸ”— Mode: ${mode}`);\n\nreturn [{\n  json: merged\n}];"
      },
      "id": "dbb673d9-e5dd-4d11-9095-c6df3f8e5e3a",
      "name": "Merge All 5 Sources",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2048,
        400
      ]
    },
    {
      "parameters": {
        "jsCode": "// Output summary for validation\nconst researcher = $input.item.json;\n\nreturn {\n  summary: {\n    name: researcher.name,\n    orcid_id: researcher.researcher_id,\n    sources_found: researcher.source_count,\n    data_quality: researcher.data_quality_score,\n    publications: researcher.metrics.publication_count,\n    citations: researcher.metrics.citation_count,\n    h_index: researcher.metrics.h_index,\n    patents: researcher.metrics.patent_count,\n    keywords_count: researcher.research_profile.keywords.length\n  },\n  \n  // Flag for next workflow\n  ready_for_enrichment: researcher.source_count >= 2 && researcher.data_quality_score >= 40,\n  \n  // Full data for next node\n  thin_object: researcher\n};"
      },
      "id": "6eaf0cd2-2f92-49a9-bad0-e9385084c99f",
      "name": "Generate Acquisition Summary",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2896,
        384
      ]
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{ $json.ready_for_enrichment }}",
              "value2": true
            }
          ]
        }
      },
      "id": "97530bcf-b5ed-4676-a425-635f3a8d210e",
      "name": "Quality Gate: Check Data Completeness",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        3136,
        416
      ]
    },
    {
      "parameters": {},
      "id": "e0a11e3b-8e0e-4294-8384-5d53cd8e01eb",
      "name": "Log Success",
      "type": "n8n-nodes-base.noOp",
      "typeVersion": 1,
      "position": [
        4144,
        208
      ]
    },
    {
      "parameters": {},
      "id": "6b0b1fb8-3695-482f-9c63-5478bb40436e",
      "name": "Log Insufficient Data",
      "type": "n8n-nodes-base.noOp",
      "typeVersion": 1,
      "position": [
        3536,
        448
      ]
    },
    {
      "parameters": {
        "numberInputs": 5
      },
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.2,
      "position": [
        1792,
        352
      ],
      "id": "d6adac92-e7e6-4c1a-9d26-3a2e73df6cb6",
      "name": "Merge"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://8b9cbf46.databases.neo4j.io/db/neo4j/query/v2",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpBasicAuth",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{$json}}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        3488,
        208
      ],
      "id": "6d7986de-6642-4b24-92dc-8c5644736039",
      "name": "Neo4j - Create Standard Node",
      "credentials": {
        "httpBasicAuth": {
          "id": "HximmqteOLptnTyu",
          "name": "Neo4j Gen 2"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "const thinObject = $input.item.json.thin_object;\n\n// Extract institution from affiliation\nconst affiliation = thinObject.affiliation || {};\nconst institution = affiliation.institution || null;\n\n// Get ORCID from researcher_id (it stores ORCID there)\nconst orcid = thinObject.researcher_id && thinObject.researcher_id !== `temp_${Date.now()}` \n  ? thinObject.researcher_id \n  : null;\n\nconst singleQuery = {\n  statement: `\n    MERGE (thin:ThinObject:Researcher {asset_id: $asset_id})\n    SET thin.status = $status,\n        thin.object_type = $object_type,\n        thin.name = $name,\n        thin.email = $email,\n        thin.orcid = $orcid,\n        thin.institution = $institution,\n        \n        //Raw source data for OBJECT-2 Pass 1 Unification\n        thin.orcid_data = $orcid_data,\n        thin.scopus_data = $scopus_data,\n        thin.scholar_data = $scholar_data,\n        thin.university_data = $university_data,\n        thin.patents_data = $patents_data,\n        \n        // Additional metadata\n        thin.researcher_id = $researcher_id,\n        thin.affiliation = $affiliation,\n        thin.urls = $urls,\n        thin.research_profile = $research_profile,\n        thin.metrics = $metrics,\n        thin.sources = $sources,\n        thin.source_count = $source_count,\n        thin.data_completeness = $data_completeness,\n        thin.enrichment_metadata = $enrichment_metadata,\n        thin.created_at = coalesce(thin.created_at, datetime()),\n        thin.updated_at = datetime()\n    RETURN thin.asset_id AS asset_id, \n           thin.name AS name, \n           thin.status AS status, \n           thin.object_type AS object_type\n  `,\n  parameters: {\n    asset_id: `thin:researcher:${thinObject.name.toLowerCase().replace(/ /g, '_')}`,\n    status: \"raw\",\n    object_type: \"researcher\",\n    name: thinObject.name,\n    email: thinObject.email,\n    orcid: orcid,\n    institution: institution,\n    researcher_id: thinObject.researcher_id,\n    \n    // Raw source data - THIS IS WHAT OBJECT-2 NEEDS\n    orcid_data: JSON.stringify({\n      orcid_id: orcid,\n      employment: thinObject.employment_history || [],\n      education: thinObject.education || [],\n      publications: thinObject.publications?.filter(p => p.source === 'orcid') || [],\n      keywords: thinObject.research_profile?.keywords || []\n    }),\n    \n    scopus_data: JSON.stringify({\n      scopus_id: thinObject.urls?.scopus ? thinObject.urls.scopus.split('authorId=')[1] : null,\n      h_index: thinObject.metrics?.h_index || 0,\n      citations: thinObject.metrics?.citation_count || 0,\n      document_count: thinObject.metrics?.publication_count || 0,\n      subject_areas: thinObject.research_profile?.subject_areas || [],\n      publications: thinObject.publications?.filter(p => p.source === 'scopus') || []\n    }),\n    \n    scholar_data: JSON.stringify({\n      scholar_id: thinObject.urls?.scholar ? thinObject.urls.scholar.split('user=')[1] : null,\n      h_index: thinObject.metrics?.h_index || 0,\n      citations: thinObject.metrics?.citation_count || 0,\n      i10_index: thinObject.metrics?.i10_index || 0,\n      publications: thinObject.publications?.filter(p => p.source === 'scholar') || [],\n      co_authors: thinObject.co_authors || [],\n      interests: thinObject.research_profile?.keywords || []\n    }),\n    \n    university_data: JSON.stringify({\n      profile_url: thinObject.urls?.deakin || null,\n      department: affiliation.department || null,\n      school: affiliation.school || null,\n      position: affiliation.position || null,\n      biography: thinObject.research_profile?.biography || null,\n      research_interests: thinObject.research_profile?.keywords || []\n    }),\n    \n    patents_data: JSON.stringify({\n      patents: thinObject.patents || [],\n      patent_count: thinObject.metrics?.patent_count || 0,\n      ipc_codes: thinObject.patent_ipc_codes || [],\n      assignees: thinObject.patent_assignees || []\n    }),\n    \n    // Keep original fields too\n    affiliation: JSON.stringify(thinObject.affiliation),\n    urls: JSON.stringify(thinObject.urls),\n    research_profile: JSON.stringify(thinObject.research_profile),\n    metrics: JSON.stringify(thinObject.metrics),\n    sources: JSON.stringify(thinObject.sources),\n    source_count: thinObject.source_count,\n    data_completeness: thinObject.data_quality_score / 100.0,\n    enrichment_metadata: JSON.stringify(thinObject.enrichment_metadata)\n  }\n};\n\nreturn {\n  json: singleQuery\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        3296,
        208
      ],
      "id": "29c53f6a-92c3-43b6-a82f-7ea613c92a6f",
      "name": "Prepare Neo4j Parameters"
    },
    {
      "parameters": {
        "jsCode": "// Parse Crawl4AI output with timestamp metadata\nconst crawlData = $input.item.json;\nconst scrapeStartTime = Date.now();\n\nconst data = Array.isArray(crawlData) ? crawlData[0] : crawlData;\n\nif (!data.success) {\n  return {\n    _node: 'Parse Deakin Crawl4AI Output',\n    deakin_found: false,\n    _source_meta: {\n      source: 'deakin',\n      scraped_at: new Date().toISOString(),\n      success: false,\n      error: data.error || 'Scrape failed',\n      url: data.url || null\n    }\n  };\n}\n\nreturn {\n  _node: 'Parse Deakin Crawl4AI Output',\n  deakin_found: true,\n  deakin_profile_url: data.url || null,\n  contact_email: data.email || null,\n  department: data.department || null,\n  school: data.school || null,\n  position: data.position || null,\n  research_interests: data.research_interests || [],\n  current_projects: data.current_projects || [],\n  expertise_keywords: data.research_interests || [],\n  biography: data.biography || null,\n  _source_meta: {\n    source: 'deakin',\n    scraped_at: new Date().toISOString(),\n    success: true,\n    url: data.url || null\n  }\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1584,
        -16
      ],
      "id": "9592aa26-4883-430d-9906-f9b08a02d6d6",
      "name": "Parse Crawl4AI Output"
    },
    {
      "parameters": {
        "jsCode": "// Fetch pending ResearcherSeeds for OBJECT-1\nconst cypherPayload = {\n  statement: `\n    MATCH (seed:ResearcherSeed)\n    WHERE seed.status = 'acquiring'\n      AND seed.web_profile_url CONTAINS 'deakin'\n    RETURN seed.name AS researcher_name,\n           seed.orcid_id AS orcid_id,\n           seed.scholar_id AS scholar_id,\n           seed.institution AS institution,\n           seed.institution_domain AS institution_domain,\n           seed.department AS department,\n           seed.web_profile_url AS web_profile_url\n    ORDER BY seed.created_at ASC\n    LIMIT $limit\n  `,\n  parameters: {\n    limit: 5\n  }\n};\nreturn [{\n  json: {\n    neo4j_payload: cypherPayload\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -976,
        352
      ],
      "id": "e777b116-5d5e-43d3-9fae-a9b32456fb0a",
      "name": "Prepare Fetch Seeds Query"
    },
    {
      "parameters": {
        "jsCode": "// Update seed status to 'acquiring' when starting\nconst researcher = $input.first().json;\n\nconst cypherPayload = {\n  statement: `\n    MATCH (seed:ResearcherSeed {name: $name})\n    SET seed.status = 'acquiring',\n        seed.acquiring_started_at = timestamp()\n    RETURN seed.name AS updated\n  `,\n  parameters: {\n    name: researcher.researcher_name\n  }\n};\n\nreturn [{\n  json: {\n    neo4j_payload: cypherPayload,\n    researcher: researcher\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -32,
        368
      ],
      "id": "09ec9472-addd-4968-90ca-52abced76019",
      "name": "Update status 'acquiring'"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://8b9cbf46.databases.neo4j.io/db/neo4j/query/v2",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpBasicAuth",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ $json.neo4j_payload }}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        176,
        368
      ],
      "id": "d960ac6c-4ea7-4c35-9e62-08a1cc1cdcf5",
      "name": "Update to Aquiring",
      "credentials": {
        "httpBasicAuth": {
          "id": "HximmqteOLptnTyu",
          "name": "Neo4j Gen 2"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Transform Neo4j response to individual researcher items\nconst response = $input.first().json;\n\nconst fields = response.data.fields;\nconst values = response.data.values;\n\n// Map each row to an object using field names\nconst researchers = values.map(row => {\n  const obj = {};\n  fields.forEach((field, index) => {\n    obj[field] = row[index];\n  });\n  return obj;\n});\n\n// Return each researcher as separate item for the loop\nreturn researchers.map(r => ({ json: r }));"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -544,
        352
      ],
      "id": "ebc44df5-c2bf-4c26-abcb-13e7e82c4248",
      "name": "Transform Neo4j Response"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://8b9cbf46.databases.neo4j.io/db/neo4j/query/v2",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpBasicAuth",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ $json.neo4j_payload }}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        -768,
        352
      ],
      "id": "b913ee98-4f40-4495-8db3-547f01e83b6c",
      "name": "Fetch Seeds HTTP node",
      "credentials": {
        "httpBasicAuth": {
          "id": "HximmqteOLptnTyu",
          "name": "Neo4j Gen 2"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Get researcher data from the Loop node\nconst input = $('Split In Batches').first().json;\n\nconst formattedResearcher = {\n  // Mode identification (Gen 2: always initial_ingest)\n  mode: 'initial_ingest',\n  \n  // Existing fields\n  researcher_name: input.researcher_name,\n  department: input.department || \"\",\n  institution_name: input.institution,\n  institution_domain: input.institution_domain,\n  web_profile_url: input.web_profile_url || \"\",\n  orcid_id: input.orcid_id,\n  scholar_id: input.scholar_id\n};\n\nreturn [{\n  json: formattedResearcher\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        448,
        368
      ],
      "id": "67363f75-da19-4462-95db-d88617dbc49a",
      "name": "Format Researcher Data"
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [
        -272,
        352
      ],
      "id": "bbf11612-e402-46f6-8462-d7b732c569fd",
      "name": "Split In Batches"
    },
    {
      "parameters": {
        "rule": {
          "interval": [
            {
              "field": "hours",
              "hoursInterval": 10
            }
          ]
        }
      },
      "type": "n8n-nodes-base.scheduleTrigger",
      "typeVersion": 1.3,
      "position": [
        -1568,
        368
      ],
      "id": "9737d16b-40eb-465a-90d5-4aefd0fba302",
      "name": "Schedule Trigger"
    },
    {
      "parameters": {
        "jsCode": "// ============================================================\n// Compute Field Hashes (BULLETPROOF VERSION - NO CRYPTO)\n// ============================================================\n\nconst mergedData = $input.first().json;\n\n// Simple hash function - NO crypto module, cannot crash\nfunction computeHash(value) {\n  let str = '';\n  \n  try {\n    if (value === undefined || value === null) {\n      str = 'null';\n    } else if (typeof value === 'string') {\n      str = value;\n    } else if (typeof value === 'number') {\n      str = String(value);\n    } else if (Array.isArray(value)) {\n      str = JSON.stringify(value);\n    } else if (typeof value === 'object') {\n      str = JSON.stringify(value);\n    } else {\n      str = String(value);\n    }\n  } catch (e) {\n    str = 'error';\n  }\n  \n  // Ensure str is ALWAYS a valid string\n  if (!str || typeof str !== 'string') {\n    str = 'empty';\n  }\n  \n  // Simple djb2 hash - cannot fail\n  let hash = 5381;\n  for (let i = 0; i < str.length; i++) {\n    hash = ((hash << 5) + hash) + str.charCodeAt(i);\n    hash = hash & hash;\n  }\n  \n  return Math.abs(hash).toString(16).padStart(8, '0');\n}\n\n// Get values safely\nconst name = mergedData.name || '';\nconst email = mergedData.email || '';\nconst orcid = mergedData.orcid || '';\nconst publications = mergedData.publications || [];\nconst collaborators = mergedData.collaborators || [];\nconst keywords = mergedData.keywords || [];\nconst affiliations = mergedData.affiliations || [];\nconst patents = mergedData.patents || [];\nconst metrics = mergedData.metrics || {};\n\n// Compute hashes\nconst currentHashes = {\n  identity: computeHash({ name: name, email: email, orcid: orcid }),\n  affiliation: computeHash(affiliations),\n  publications: computeHash({\n    count: publications.length,\n    titles: publications.map(function(p) { return (p && p.title) ? p.title : ''; }).sort()\n  }),\n  metrics: computeHash(metrics),\n  patents: computeHash(patents),\n  collaborators: computeHash(collaborators.slice(0, 50).sort()),\n  keywords: computeHash(keywords.sort())\n};\n\n// Checksum\nconst checksum = computeHash(Object.values(currentHashes).join('-'));\n\n// Mode and existing hashes\nconst mode = mergedData.mode || 'initial_acquisition';\nconst existingHashes = mergedData.existing_field_hashes || {};\n\n// Find changed fields\nlet changedFields = [];\nlet verificationStatus = 'initial_ingest';\n\nif (mode === 'maintenance_scan' && Object.keys(existingHashes).length > 0) {\n  for (const field of Object.keys(currentHashes)) {\n    if (existingHashes[field] && currentHashes[field] !== existingHashes[field]) {\n      changedFields.push(field);\n    }\n  }\n  verificationStatus = changedFields.length > 0 ? 'changes_detected' : 'verified_unchanged';\n}\n\n// Check sources\nconst sourceSuccess = (mergedData.source_metadata && mergedData.source_metadata.source_success) || {};\nlet successCount = 0;\nfor (const key of Object.keys(sourceSuccess)) {\n  if (sourceSuccess[key] === true) successCount++;\n}\nif (successCount === 0) {\n  verificationStatus = 'verification_failed';\n}\n\n// Build scan_result\nlet scanResult = null;\nif (mode === 'maintenance_scan') {\n  scanResult = {\n    scan_status: verificationStatus,\n    changed_fields: changedFields,\n    change_count: changedFields.length,\n    new_field_hashes: currentHashes,\n    existing_field_hashes: existingHashes,\n    scanned_at: new Date().toISOString()\n  };\n}\n\nconsole.log('Hashes computed for: ' + name);\nconsole.log('Mode: ' + mode + ', Status: ' + verificationStatus);\n\nreturn [{\n  json: {\n    ...mergedData,\n    source_verification: {\n      status: verificationStatus,\n      changes_detected: changedFields.length > 0,\n      changed_fields: changedFields,\n      change_count: changedFields.length\n    },\n    field_hashes: currentHashes,\n    scan_result: scanResult,\n    hash_metadata: {\n      computed_at: new Date().toISOString(),\n      algorithm: 'djb2',\n      field_count: Object.keys(currentHashes).length\n    }\n  }\n}];"
      },
      "id": "e26c07b0-ec13-4eca-b80e-1ed8ab74627a",
      "name": "Compute Field Hashes",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2592,
        384
      ]
    },
    {
      "parameters": {
        "jsCode": "// Prepare Supabase Payload - Definition Card compliant\nconst thinObject = $('Compute Field Hashes').first().json;\n\nconst supabasePayload = [{\n  asset_id: `thin:researcher:${thinObject.name.toLowerCase().replace(/ /g, '_')}`,\n  checksum: thinObject.field_hashes?.checksum || thinObject.source_verification?.checksum,\n  version: 1,\n  job_id: `acq_${Date.now()}`,\n  status: 'raw'\n}];\n\nreturn [{\n  json: {\n    supabase_payload: supabasePayload\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        3696,
        208
      ],
      "id": "e6c7856e-702a-40c0-b8e6-63a7a4631057",
      "name": "Prepare PostgreSQL Payload"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://esgwrqcyhtzcsbepvqhc.supabase.co/rest/v1/thin_object_records?on_conflict=asset_id",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "supabaseApi",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            },
            {
              "name": "Prefer",
              "value": "resolution=merge-duplicates, return=representation"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ $json.supabase_payload }}",
        "options": {
          "timeout": 30000
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        3904,
        208
      ],
      "id": "7ca8970e-71a0-4ce1-ab23-1a77b1aa7d5c",
      "name": "Store in Supabase(Postgre)",
      "credentials": {
        "qdrantRestApi": {
          "id": "5kvIJD32UkQxRAis",
          "name": "Qdrant account"
        },
        "supabaseApi": {
          "id": "vTKATqGswB3PHE6S",
          "name": "Supabase account"
        }
      }
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "object1-maintenance-scan",
        "responseMode": "responseNode",
        "options": {}
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2.1,
      "position": [
        -1728,
        560
      ],
      "id": "f50b67a5-c36e-4a0d-b4aa-85cf3599163b",
      "name": "GOVERN-1 Webhook Trigger",
      "webhookId": "6c4cd209-2384-416b-8d5b-da5ec39b5046"
    },
    {
      "parameters": {
        "rules": {
          "values": [
            {
              "conditions": {
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "strict",
                  "version": 2
                },
                "conditions": [
                  {
                    "leftValue": "={{ $json.mode }}",
                    "rightValue": "maintenance_scan",
                    "operator": {
                      "type": "string",
                      "operation": "notEquals"
                    },
                    "id": "a9cd5b81-25c3-44fd-a284-384266fdfa5e"
                  }
                ],
                "combinator": "and"
              }
            },
            {
              "conditions": {
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "strict",
                  "version": 2
                },
                "conditions": [
                  {
                    "id": "06f9692e-33f8-41ce-866d-6cb651f95841",
                    "leftValue": "={{ $json.mode }}",
                    "rightValue": "maintenance_scan",
                    "operator": {
                      "type": "string",
                      "operation": "equals"
                    }
                  }
                ],
                "combinator": "and"
              }
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.switch",
      "typeVersion": 3.3,
      "position": [
        -1280,
        368
      ],
      "id": "f386973d-7733-4ea0-a6d1-7baff37f7a0f",
      "name": "Route by Mode"
    },
    {
      "parameters": {
        "jsCode": "// ============================================================\n// Node: Prepare Maintenance Scan\n// Purpose: Format GOVERN-1 webhook input for source fetching\n// Input: Data from GOVERN-1 webhook (inside body property)\n// Output: Formatted for parallel source fetches + hash comparison\n// ============================================================\n\nconst webhookData = $input.first().json;\n\n// Extract the body - this is where the actual data lives\nconst input = webhookData.body || webhookData;\n\n// Validate required fields\nif (!input.asset_id) {\n  throw new Error(`Missing required field: asset_id. Received keys: ${Object.keys(input).join(', ')}`);\n}\n\nif (!input.name) {\n  throw new Error(`Missing required field: name. Received keys: ${Object.keys(input).join(', ')}`);\n}\n\n// Extract researcher identifiers from GOVERN-1 payload\nconst assetId = input.asset_id;\nconst researcherName = input.name;\nconst orcidId = input.orcid || null;\nconst institution = input.institution || null;\nconst scholarId = input.scholar_id || null;\nconst email = input.email || null;\n\n// Extract existing field hashes for change comparison\nconst existingFieldHashes = input.existing_field_hashes || {};\n\n// Log scan initiation\nconsole.log(`ðŸ” Maintenance scan initiated for: ${researcherName} (${assetId})`);\nconsole.log(`ðŸ“Š Existing hashes to compare: ${Object.keys(existingFieldHashes).length} fields`);\n\nreturn [{\n  json: {\n    // ============================================================\n    // Researcher identifiers (passed to source fetch nodes)\n    // ============================================================\n    asset_id: assetId,\n    researcher_name: researcherName,\n    orcid_id: orcidId,\n    institution: institution,\n    scholar_id: scholarId,\n    email: email,\n    \n    // ============================================================\n    // Mode flag - tells downstream nodes this is a maintenance scan\n    // ============================================================\n    mode: 'maintenance_scan',\n    \n    // ============================================================\n    // Existing hashes for comparison after source fetch\n    // Structure: { field_name: hash_value, ... }\n    // ============================================================\n    existing_field_hashes: existingFieldHashes,\n    \n    // ============================================================\n    // Control flags\n    // ============================================================\n    skip_status_update: true,\n    skip_neo4j_storage: true,\n    skip_qdrant_storage: true,\n    skip_supabase_storage: true,\n    return_scan_result: true,\n    \n    // ============================================================\n    // Scan metadata\n    // ============================================================\n    scan_initiated_at: new Date().toISOString(),\n    scan_source: 'GOVERN-1'\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1520,
        560
      ],
      "id": "40b96fe4-3e04-45b8-a23f-4c40c907eebe",
      "name": "Prepare Maintenance Scan"
    },
    {
      "parameters": {
        "rules": {
          "values": [
            {
              "conditions": {
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "strict",
                  "version": 2
                },
                "conditions": [
                  {
                    "leftValue": "={{ $json.mode }}",
                    "rightValue": "maintenance_scan",
                    "operator": {
                      "type": "string",
                      "operation": "notEquals"
                    },
                    "id": "a9cd5b81-25c3-44fd-a284-384266fdfa5e"
                  }
                ],
                "combinator": "and"
              }
            },
            {
              "conditions": {
                "options": {
                  "caseSensitive": true,
                  "leftValue": "",
                  "typeValidation": "strict",
                  "version": 2
                },
                "conditions": [
                  {
                    "id": "b8c959b9-877c-4c06-8d8e-2c40a2c5e24c",
                    "leftValue": "={{ $json.mode }}",
                    "rightValue": "maintenance_scan",
                    "operator": {
                      "type": "string",
                      "operation": "equals"
                    }
                  }
                ],
                "combinator": "and"
              }
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.switch",
      "typeVersion": 3.3,
      "position": [
        2288,
        400
      ],
      "id": "9dfc5a65-f07f-401d-949d-51912d21abd3",
      "name": "Route Merge"
    },
    {
      "parameters": {
        "jsCode": "// ============================================================\n// Node: Format Scan Result\n// Purpose: Format the scan result to return to GOVERN-1 webhook\n// Input: Merged data with scan_result from maintenance scan\n// Output: Structured response for GOVERN-1\n// ============================================================\n\nconst merged = $input.first().json;\n\n// Validate this is a maintenance scan result\nif (merged.mode !== 'maintenance_scan') {\n  throw new Error('Format Scan Result called but mode is not maintenance_scan');\n}\n\nif (!merged.scan_result) {\n  throw new Error('Missing scan_result in merged data');\n}\n\nconst scanResult = merged.scan_result;\n\n// Build response for GOVERN-1\nconst response = {\n  // Core identification\n  asset_id: merged.asset_id,\n  name: merged.name,\n  orcid: merged.orcid,\n  institution: merged.institution,\n  \n  // Scan outcome\n  scan_status: scanResult.scan_status,\n  changed_fields: scanResult.changed_fields,\n  change_count: scanResult.change_count,\n  \n  // Hash data for future comparisons\n  new_field_hashes: scanResult.new_field_hashes,\n  previous_field_hashes: scanResult.existing_field_hashes,\n  \n  // Change details (for severity calculation in GOVERN-2)\n  change_details: scanResult.change_details,\n  \n  // Metrics snapshot (for GOVERN-2 severity calculation)\n  metrics_snapshot: {\n    h_index: merged.metrics?.h_index || 0,\n    total_citations: merged.metrics?.total_citations || 0,\n    publication_count: merged.metrics?.publication_count || 0,\n    collaborator_count: merged.metrics?.collaborator_count || 0,\n    patent_count: merged.metrics?.patent_count || 0\n  },\n  \n  // Fresh data (only if changes detected - for GOVERN-3)\n  fresh_data: merged.fresh_data || null,\n  \n  // Data quality\n  data_completeness: merged.data_completeness,\n  data_quality_flags: merged.data_quality_flags,\n  \n  // Source metadata\n  sources_scraped: merged.source_metadata?.sources_scraped || [],\n  source_success: merged.source_metadata?.source_success || {},\n  \n  // Timing\n  scan_initiated_at: scanResult.scan_initiated_at,\n  scanned_at: scanResult.scanned_at,\n  scan_duration_ms: scanResult.scan_duration_ms,\n  \n  // Response metadata\n  response_type: 'maintenance_scan_result',\n  workflow: 'OBJECT-1',\n  workflow_version: 'v3.1_GOVERN'\n};\n\nconsole.log(`ðŸ“¤ Returning scan result to GOVERN-1`);\nconsole.log(`   Status: ${response.scan_status}`);\nconsole.log(`   Changes: ${response.change_count} fields`);\n\nreturn [{\n  json: response\n}];"
      },
      "id": "54dfaeed-599f-43df-9c3b-61920031377c",
      "name": "Format Scan Result",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2784,
        656
      ]
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.4,
      "position": [
        2992,
        656
      ],
      "id": "32a1254e-d63e-4ed9-bcd2-585c95e33ec9",
      "name": "Respond to Webhook"
    },
    {
      "parameters": {
        "jsCode": "// ============================================================\n// Node: Format Maintenance Data\n// Purpose: Format maintenance scan data for source fetching\n// This node is ONLY used in the maintenance scan path\n// ============================================================\n\nconst input = $input.first().json;\n\n// Pass through the data formatted for source nodes\n// The field names match what the source fetch nodes expect\nreturn [{\n  json: {\n    // Fields that source nodes need\n    researcher_name: input.researcher_name,\n    orcid_id: input.orcid_id,\n    institution: input.institution,\n    scholar_id: input.scholar_id,\n    email: input.email,\n    \n    // Maintenance scan context (passed through to Merge)\n    asset_id: input.asset_id,\n    mode: 'maintenance_scan',\n    existing_field_hashes: input.existing_field_hashes || {},\n    \n    // Control flags (passed through to Merge and storage nodes)\n    skip_status_update: true,\n    skip_neo4j_storage: true,\n    skip_qdrant_storage: true,\n    skip_supabase_storage: true,\n    return_scan_result: true,\n    scan_initiated_at: input.scan_initiated_at,\n    scan_source: input.scan_source\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        448,
        560
      ],
      "id": "970cc38d-ef7f-4884-96d0-c22740b4ade0",
      "name": "Format Maintenance Data"
    }
  ],
  "pinData": {
    "Manual Trigger - Input Researcher Data": [
      {
        "json": {
          "researcher_name": "Colin Barrow",
          "department": "School of Life & Env Sciences",
          "institution_name": "Deakin University",
          "institution_domain": "deakin.edu.au",
          "orcid_id": "0000-0002-2153-7267",
          "scholar_id": "ol6Y8nUAAAAJ&hl"
        }
      }
    ],
    "GOVERN-1 Webhook Trigger": [
      {
        "json": {
          "headers": {
            "host": "mbcrc.app.n8n.cloud",
            "user-agent": "axios/1.12.0",
            "content-length": "181",
            "accept": "application/json,text/html,application/xhtml+xml,application/xml,text/*;q=0.9, image/*;q=0.8, */*;q=0.7",
            "accept-encoding": "gzip, br",
            "cdn-loop": "cloudflare; loops=1; subreqs=1",
            "cf-connecting-ip": "20.218.238.113",
            "cf-ew-via": "15",
            "cf-ipcountry": "DE",
            "cf-ray": "9b10f806b3b03801-FRA",
            "cf-visitor": "{\"scheme\":\"https\"}",
            "cf-worker": "n8n.cloud",
            "content-type": "application/json",
            "x-forwarded-for": "20.218.238.113, 172.68.193.140",
            "x-forwarded-host": "mbcrc.app.n8n.cloud",
            "x-forwarded-port": "443",
            "x-forwarded-proto": "https",
            "x-forwarded-server": "traefik-prod-users-gwc-42-bd54959cf-cbnqh",
            "x-is-trusted": "yes",
            "x-real-ip": "20.218.238.113"
          },
          "params": {},
          "query": {},
          "body": {
            "mode": "maintenance_scan",
            "asset_id": "fat:researcher:colin_barrow",
            "name": "Colin Barrow",
            "orcid": "0000-0002-1825-0097",
            "institution": "Deakin University",
            "existing_field_hashes": {}
          },
          "webhookUrl": "https://mbcrc.app.n8n.cloud/webhook/object1-maintenance-scan",
          "executionMode": "production"
        }
      }
    ],
    "Schedule Trigger": [
      {
        "json": {
          "timestamp": "2025-12-21T20:00:23.006+11:00",
          "Readable date": "December 21st 2025, 8:00:23 pm",
          "Readable time": "8:00:23 pm",
          "Day of week": "Sunday",
          "Year": "2025",
          "Month": "December",
          "Day of month": "21",
          "Hour": "20",
          "Minute": "00",
          "Second": "23",
          "Timezone": "Australia/Melbourne (UTC+11:00)"
        }
      }
    ]
  },
  "connections": {
    "Manual Trigger - Input Researcher Data": {
      "main": [
        []
      ]
    },
    "Source 1: Search Deakin Profile": {
      "main": [
        [
          {
            "node": "Extract Deakin Profile URL",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Extract Deakin Profile URL": {
      "main": [
        [
          {
            "node": "Crawl4AI: Scrape Deakin Profile",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Crawl4AI: Scrape Deakin Profile": {
      "main": [
        [
          {
            "node": "Parse Crawl4AI Output",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Search ORCID by Name": {
      "main": [
        [
          {
            "node": "Extract ORCID ID",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Extract ORCID ID": {
      "main": [
        [
          {
            "node": "Source 2: Fetch ORCID Full Record",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Source 2: Fetch ORCID Full Record": {
      "main": [
        [
          {
            "node": "Parse ORCID Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse ORCID Data": {
      "main": [
        [
          {
            "node": "Merge",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Search Scopus by ORCID": {
      "main": [
        [
          {
            "node": "Parse Scopus Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse Scopus Data": {
      "main": [
        [
          {
            "node": "Merge",
            "type": "main",
            "index": 2
          }
        ]
      ]
    },
    "Source 4: Fetch Google Scholar Profile": {
      "main": [
        [
          {
            "node": "Parse Google Scholar Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse Google Scholar Data": {
      "main": [
        [
          {
            "node": "Merge",
            "type": "main",
            "index": 3
          }
        ]
      ]
    },
    "Source 5: Search Google Patents": {
      "main": [
        [
          {
            "node": "Parse Patent Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse Patent Data": {
      "main": [
        [
          {
            "node": "Merge",
            "type": "main",
            "index": 4
          }
        ]
      ]
    },
    "Merge All 5 Sources": {
      "main": [
        [
          {
            "node": "Route Merge",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Generate Acquisition Summary": {
      "main": [
        [
          {
            "node": "Quality Gate: Check Data Completeness",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Quality Gate: Check Data Completeness": {
      "main": [
        [
          {
            "node": "Prepare Neo4j Parameters",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Log Insufficient Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge": {
      "main": [
        [
          {
            "node": "Merge All 5 Sources",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Neo4j - Create Standard Node": {
      "main": [
        [
          {
            "node": "Prepare PostgreSQL Payload",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Neo4j Parameters": {
      "main": [
        [
          {
            "node": "Neo4j - Create Standard Node",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse Crawl4AI Output": {
      "main": [
        [
          {
            "node": "Merge",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Fetch Seeds Query": {
      "main": [
        [
          {
            "node": "Fetch Seeds HTTP node",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Update status 'acquiring'": {
      "main": [
        [
          {
            "node": "Update to Aquiring",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Fetch Seeds HTTP node": {
      "main": [
        [
          {
            "node": "Transform Neo4j Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Transform Neo4j Response": {
      "main": [
        [
          {
            "node": "Split In Batches",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Update to Aquiring": {
      "main": [
        [
          {
            "node": "Format Researcher Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split In Batches": {
      "main": [
        [],
        [
          {
            "node": "Update status 'acquiring'",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Researcher Data": {
      "main": [
        [
          {
            "node": "Source 1: Search Deakin Profile",
            "type": "main",
            "index": 0
          },
          {
            "node": "Search ORCID by Name",
            "type": "main",
            "index": 0
          },
          {
            "node": "Search Scopus by ORCID",
            "type": "main",
            "index": 0
          },
          {
            "node": "Source 4: Fetch Google Scholar Profile",
            "type": "main",
            "index": 0
          },
          {
            "node": "Source 5: Search Google Patents",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Schedule Trigger": {
      "main": [
        [
          {
            "node": "Route by Mode",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Log Success": {
      "main": [
        [
          {
            "node": "Split In Batches",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Compute Field Hashes": {
      "main": [
        [
          {
            "node": "Generate Acquisition Summary",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare PostgreSQL Payload": {
      "main": [
        [
          {
            "node": "Store in Supabase(Postgre)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Store in Supabase(Postgre)": {
      "main": [
        [
          {
            "node": "Log Success",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "GOVERN-1 Webhook Trigger": {
      "main": [
        [
          {
            "node": "Prepare Maintenance Scan",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Route by Mode": {
      "main": [
        [
          {
            "node": "Prepare Fetch Seeds Query",
            "type": "main",
            "index": 0
          }
        ],
        []
      ]
    },
    "Prepare Maintenance Scan": {
      "main": [
        [
          {
            "node": "Route by Mode",
            "type": "main",
            "index": 0
          },
          {
            "node": "Format Maintenance Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Route Merge": {
      "main": [
        [
          {
            "node": "Compute Field Hashes",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Format Scan Result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Scan Result": {
      "main": [
        [
          {
            "node": "Respond to Webhook",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Maintenance Data": {
      "main": [
        [
          {
            "node": "Source 1: Search Deakin Profile",
            "type": "main",
            "index": 0
          },
          {
            "node": "Search ORCID by Name",
            "type": "main",
            "index": 0
          },
          {
            "node": "Search Scopus by ORCID",
            "type": "main",
            "index": 0
          },
          {
            "node": "Source 4: Fetch Google Scholar Profile",
            "type": "main",
            "index": 0
          },
          {
            "node": "Source 5: Search Google Patents",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "66df6c3f-c84a-4303-a02b-378cfef4fbb9",
  "meta": {
    "instanceId": "bc0e623ae1bb3524870487de3c7fa60f3a571019006cc26dd79ca981250a48aa"
  },
  "id": "pHKkvilFNxcgNn1Q",
  "tags": [
    {
      "updatedAt": "2025-11-19T16:16:22.409Z",
      "createdAt": "2025-11-19T16:16:22.409Z",
      "id": "M2HE4sAVj28MumnE",
      "name": "Gen 2"
    },
    {
      "updatedAt": "2025-12-20T15:31:43.586Z",
      "createdAt": "2025-12-20T15:31:43.586Z",
      "id": "V0dxUl9wqFhzYH4e",
      "name": "Governance"
    },
    {
      "updatedAt": "2025-11-19T16:16:22.383Z",
      "createdAt": "2025-11-19T16:16:22.383Z",
      "id": "fwnhwF7AOAkcks72",
      "name": "Data Acquisition"
    }
  ]
}